{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb72817fc30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision.models import resnet18\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_data = CIFAR10(\n",
    "        root=\"data\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "    )\n",
    "test_data = CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, testloader, criterion, device):\n",
    "    model.eval()\n",
    "    print(\"Validation\")\n",
    "    valid_running_loss = 0.0\n",
    "    valid_running_correct = 0\n",
    "    counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in tqdm(enumerate(testloader), total=len(testloader)):\n",
    "            counter += 1\n",
    "\n",
    "            image, labels = data\n",
    "            image = image.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass.\n",
    "            outputs = model(image)\n",
    "            # Calculate the loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_running_loss += loss.item()\n",
    "            # Calculate the accuracy.\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            valid_running_correct += (preds == labels).sum().item()\n",
    "\n",
    "    # Loss and accuracy for the complete epoch.\n",
    "    epoch_loss = valid_running_loss / counter\n",
    "    epoch_acc = 100.0 * (valid_running_correct / len(testloader.dataset))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_448940/2698396606.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(path))\n"
     ]
    }
   ],
   "source": [
    "path = 'data/weights_1.pt'\n",
    "\n",
    "model = resnet18().to(device)\n",
    "model.fc = ClassifierHead().to(device)\n",
    "model.load_state_dict(torch.load(path))\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:01<00:00, 124.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.1781141660205878, 82.37)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, test_dataloader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5\n",
    "    plt.imshow(np.transpose(img, (1,2,0)))\n",
    "\n",
    "classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\",\n",
    "          \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import GaussianNoise\n",
    "\n",
    "class GHatDetermined(nn.Module):\n",
    "    '''\n",
    "    GHat model with fixed values epsilon[i], i = 1...N\n",
    "    '''\n",
    "    def __init__(self, model, sigma=0.1, n_samples=100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.noiser = GaussianNoise(mean=0, sigma=sigma)\n",
    "        self.model = model.to(device)\n",
    "        self.epsilons = [\n",
    "            self.noiser(torch.zeros(3, 32, 32)).to(device)\n",
    "            for _ in range(self.n_samples)\n",
    "        ]\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        probs = torch.zeros(1, 10).to(device)\n",
    "        probs.requires_grad = True\n",
    "        for i in range(self.n_samples):\n",
    "            new_x = x + self.epsilons[i]\n",
    "            probs = probs + self.softmax(self.model(new_x))\n",
    "        probs /= self.n_samples\n",
    "        return probs\n",
    "\n",
    "class GHat:\n",
    "    def __init__(self, model, sigma=0.1, n_samples=100):\n",
    "        self.noiser = GaussianNoise(mean=0.0, sigma=sigma)\n",
    "        self.n_samples = n_samples\n",
    "        self.model = model.to(device)\n",
    "        self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, image):\n",
    "        image = image.to(device)\n",
    "        if len(image.shape) == 3:\n",
    "            image = image.unsqueeze(dim=0)\n",
    "        probs = torch.zeros(1, 10).to(device)\n",
    "        for iteration in range(self.n_samples):\n",
    "            probs += self.softmax(self.model(self.noiser(image)))\n",
    "        probs /= self.n_samples\n",
    "        return probs\n",
    "    \n",
    "    def __call__(self, image):\n",
    "        return self.forward(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghat = GHat(model, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.v2 import GaussianNoise\n",
    "\n",
    "def random_attack(model, image, true_label, noiser=GaussianNoise(mean=0, sigma=0.05), n_tries=100):\n",
    "    true_label = model(image.unsqueeze(dim=0).to(device)).argmax().item()\n",
    "    for attempt in range(1, n_tries + 1):\n",
    "        new_image = noiser(image)\n",
    "        new_image = new_image.unsqueeze(dim=0).to(device)\n",
    "        label = model(new_image).argmax().item()\n",
    "        if label != true_label:\n",
    "            new_image = new_image.squeeze(0).detach()\n",
    "            return label, new_image, attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(model, image, true_label, epsilon=0.01):\n",
    "    model.eval()\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(dim=0)\n",
    "    image = image.to(device)\n",
    "    image.requires_grad = True\n",
    "    \n",
    "    output = model(image)\n",
    "    loss = criterion(model(image), torch.tensor([true_label]).to(device))\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    image_grad = image.grad\n",
    "    \n",
    "    bad_image = image + epsilon * image_grad.sign()\n",
    "    bad_image = torch.clamp(bad_image, 0, 1)\n",
    "\n",
    "    label = model(bad_image).argmax().item()\n",
    "    has_attack_worked = (label != true_label)\n",
    "    \n",
    "    bad_image = bad_image.squeeze(0).detach()\n",
    "    \n",
    "    return has_attack_worked, bad_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient_estimation(model, image, true_label, n_samples=10):\n",
    "    if len(image.shape) == 3:\n",
    "        image = image.unsqueeze(dim=0)\n",
    "    image.requires_grad = True\n",
    "    loss_fn = nn.NLLLoss()\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        gdet = GHatDetermined(model)\n",
    "        log_prob = torch.log(gdet(image))\n",
    "        loss = loss_fn(log_prob, torch.tensor([true_label]).to(device))\n",
    "        loss.backward()\n",
    "        model.zero_grad()\n",
    "    grad_estimation = image.grad / n_samples\n",
    "    return grad_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gradient_estimation_attack(\n",
    "        model,\n",
    "        model_to_attack,\n",
    "        image,\n",
    "        true_label,\n",
    "        epsilon_mult=1,\n",
    "        epsilon=0.04,\n",
    "        n_samples=10,\n",
    "        eps_limits = (10**(-6), 100)\n",
    "    ):\n",
    "    model.eval()\n",
    "\n",
    "    image = image.to(device)\n",
    "    gradient_estimation = get_gradient_estimation(model, image, true_label, n_samples).to(device)\n",
    "    bad_image = image + epsilon * gradient_estimation.sign()\n",
    "    bad_image = torch.clamp(bad_image, 0, 1)\n",
    "    new_label = model_to_attack(bad_image).argmax().item()\n",
    "    true_label = true_label.item()\n",
    "\n",
    "    if epsilon_mult == 1:\n",
    "        bad_image = bad_image.squeeze(0).detach()\n",
    "        return new_label != true_label, bad_image, new_label, epsilon\n",
    "    elif epsilon_mult > 1:\n",
    "        while new_label == true_label and epsilon < eps_limits[1]:\n",
    "            epsilon *= epsilon_mult\n",
    "            bad_image = image + epsilon * gradient_estimation.sign()\n",
    "            bad_image = torch.clamp(bad_image, 0, 1)\n",
    "            new_label = model_to_attack(bad_image).argmax().item()\n",
    "        bad_image = bad_image.squeeze(0).detach()\n",
    "        return eps_limits[0] < epsilon < eps_limits[1],  bad_image, new_label, epsilon\n",
    "    else:\n",
    "        while new_label != true_label and eps_limits[0] < epsilon:\n",
    "            epsilon *= epsilon_mult\n",
    "            bad_image = image + epsilon * gradient_estimation.sign()\n",
    "            bad_image = torch.clamp(bad_image, 0, 1)\n",
    "            new_label = model_to_attack(bad_image).argmax().item()\n",
    "        bad_image = image - epsilon * gradient_estimation.sign()\n",
    "        bad_image = bad_image.squeeze(0).detach()\n",
    "        return eps_limits[0] < epsilon < eps_limits[1], bad_image, new_label, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Атакуем по-умному"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_batch, labels_batch = next(iter(test_dataloader))\n",
    "image = images_batch[5]\n",
    "true_label = labels_batch[5].item()\n",
    "imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Атакуем и строим зависимость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_gap=0.27448, BAD eps=7.62939453125e-07\n",
      "p_gap=0.80935, BAD eps=7.62939453125e-07\n",
      "p_gap=0.63897, p_a=0.78289, p_b=0.14392. eps=0.003125. p_gap/eps=204.4696283340454. horse -> horse\n",
      "p_gap=0.47452, BAD eps=7.62939453125e-07\n",
      "p_gap=0.43589, BAD eps=7.62939453125e-07\n",
      "p_gap=0.81737, p_a=0.90869, p_b=0.09131. eps=0.003125. p_gap/eps=261.55993938446045. automobile -> automobile\n",
      "p_gap=0.88259, BAD eps=7.62939453125e-07\n",
      "p_gap=0.86568, p_a=0.93127, p_b=0.06559. eps=0.0015625. p_gap/eps=554.0328884124756. cat -> cat\n",
      "p_gap=0.88339, p_a=0.92946, p_b=0.04607. eps=0.00625. p_gap/eps=141.34210348129272. bird -> bird\n",
      "p_gap=0.59689, BAD eps=7.62939453125e-07\n",
      "p_gap=0.63494, p_a=0.81745, p_b=0.18251. eps=0.00078125. p_gap/eps=812.7257347106934. frog -> frog\n",
      "p_gap=0.5949, BAD eps=7.62939453125e-07\n",
      "p_gap=0.49559, BAD eps=7.62939453125e-07\n",
      "p_gap=0.69873, p_a=0.84936, p_b=0.15063. eps=0.00625. p_gap/eps=111.79670810699463. frog -> frog\n",
      "p_gap=0.11589, BAD eps=7.62939453125e-07\n",
      "p_gap=0.78156, p_a=0.89066, p_b=0.10911. eps=0.0125. p_gap/eps=62.52453625202179. deer -> deer\n",
      "p_gap=0.65458, p_a=0.7953, p_b=0.14072. eps=0.00625. p_gap/eps=104.73286151885986. airplane -> airplane\n",
      "p_gap=0.84135, BAD eps=7.62939453125e-07\n",
      "p_gap=0.33949, p_a=0.66974, p_b=0.33025. eps=0.003125. p_gap/eps=108.63707542419434. truck -> truck\n",
      "p_gap=0.61081, BAD eps=7.62939453125e-07\n",
      "p_gap=0.81506, p_a=0.88789, p_b=0.07282. eps=0.000390625. p_gap/eps=2086.5614891052246. airplane -> airplane\n",
      "p_gap=0.8526, BAD eps=7.62939453125e-07\n",
      "p_gap=0.70474, BAD eps=7.62939453125e-07\n",
      "p_gap=0.1758, BAD eps=7.62939453125e-07\n",
      "p_gap=0.21859, p_a=0.6093, p_b=0.3907. eps=0.00078125. p_gap/eps=279.79610443115234. automobile -> automobile\n",
      "p_gap=0.37127, BAD eps=7.62939453125e-07\n",
      "p_gap=0.76982, p_a=0.8755, p_b=0.10568. eps=0.00625. p_gap/eps=123.1714940071106. cat -> cat\n",
      "p_gap=0.85037, BAD eps=7.62939453125e-07\n",
      "p_gap=0.81791, p_a=0.90895, p_b=0.09104. eps=0.00625. p_gap/eps=130.86561799049377. horse -> horse\n",
      "p_gap=0.24121, p_a=0.61972, p_b=0.37851. eps=0.0125. p_gap/eps=19.296445846557617. deer -> deer\n",
      "p_gap=0.71679, p_a=0.85147, p_b=0.13468. eps=0.003125. p_gap/eps=229.3740940093994. deer -> deer\n",
      "p_gap=0.74953, p_a=0.87476, p_b=0.12523. eps=0.000390625. p_gap/eps=1918.7956237792969. cat -> cat\n",
      "p_gap=0.77651, BAD eps=7.62939453125e-07\n",
      "p_gap=0.534, BAD eps=7.62939453125e-07\n"
     ]
    }
   ],
   "source": [
    "# ищем картинку с наим p_a-p_b\n",
    "eps_dependence = [[], [], []]  # [[p_gap_1, p_gap_2, ...], [eps_1, eps_2, ...], [certified_radius_1, certified_radius_2, ...]]\n",
    "eps_limits = (10**(-6), 10)\n",
    "phi_inv = lambda x: torch.distributions.Normal(0, 1).icdf(torch.tensor(x))\n",
    "\n",
    "cnt_success = 0\n",
    "model_to_attack = GHat(model)\n",
    "for image_batch, label_batch in test_dataloader:\n",
    "    image_batch = image_batch.to(device)\n",
    "    label_batch = label_batch.to(device)\n",
    "\n",
    "    for image, label in zip(image_batch, label_batch):\n",
    "        eps = 0.05\n",
    "        imshow(image.cpu())\n",
    "        p_sorted = (nn.Softmax()(model(image.unsqueeze(dim=0)))[0]).sort(descending=True)\n",
    "        if p_sorted.indices[0].item() != label.item():\n",
    "            continue\n",
    "        p_a = p_sorted.values[0].item()  # a_class = p_sorted.indices[0].item()\n",
    "        p_b = p_sorted.values[1].item()\n",
    "        p_gap = p_a-p_b\n",
    "\n",
    "        if p_gap > 0.9:\n",
    "            continue\n",
    "        \n",
    "        is_successful, bad_image, label_predicted, _ = find_gradient_estimation_attack(model, model_to_attack, image, label, epsilon=eps)\n",
    "        \n",
    "        if is_successful: # class changed\n",
    "            eps_mult = 0.5\n",
    "        else: # same class\n",
    "            eps_mult = 2\n",
    "\n",
    "        is_successful, bad_image, label_predicted, eps = find_gradient_estimation_attack(model, model_to_attack, image, label, eps_mult, epsilon=eps)\n",
    "        \n",
    "        if is_successful:\n",
    "            print(f'p_gap={round(p_gap, 5)}, p_a={round(p_a, 5)}, p_b={round(p_b, 5)}. eps={eps}. p_gap/eps={p_gap / eps}. {classes[label.item()]} -> {classes[label_predicted]}')\n",
    "            eps_dependence[0].append(p_gap)\n",
    "            eps_dependence[1].append(eps)\n",
    "            eps_dependence[2].append(0.1 / 2 * (phi_inv(p_a) - phi_inv(p_b)).item())\n",
    "            cnt_success +=1 \n",
    "            if cnt_success > 10000:\n",
    "                break\n",
    "        else: \n",
    "            print(f'p_gap={round(p_gap, 5)}, BAD eps={eps}')\n",
    "    with open('clever_attack.txt', 'w') as f:\n",
    "        for vector in eps_dependence:\n",
    "            line = ', '.join(map(str, vector))\n",
    "            f.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(eps_dependence[0], eps_dependence[1], s=5)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
